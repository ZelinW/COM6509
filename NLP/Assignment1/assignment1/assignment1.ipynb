{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COM6513] Assignment 1: Text Classification with Logistic Regression\n",
    "\n",
    "### Instructor: Nikos Aletras\n",
    "\n",
    "\n",
    "The goal of this assignment is to develop and test two text classification systems: \n",
    "\n",
    "- **Task 1:** sentiment analysis, in particular to predict the sentiment of movie reviews, i.e. positive or negative (binary classification).\n",
    "- **Task 2:** topic classification, to predict whether a news article is about International issues, Sports or Business (multi-class classification).\n",
    "\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "\n",
    "- Text processing methods for extracting Bag-Of-Word features, using \n",
    "    - n-grams (BOW), i.e. unigrams, bigrams and trigrams to obtain vector representations of documents where n=1,2,3 respectively. Two vector weighting schemes should be tested: (1) raw frequencies (**3 marks**); (2) tf.idf (**1 mark**). \n",
    "    - character n-grams (BOCN). A character n-gram is a contiguous sequence of characters given a word, e.g. for n=2, 'coffee' is split into {'co', 'of', 'ff', 'fe', 'ee'}. Two vector weighting schemes should be tested: (1) raw frequencies (**3 marks**); (2) tf.idf (**1 mark**). **Tip: Note the large vocabulary size!** \n",
    "    - a combination of the two vector spaces (n-grams and character n-grams) choosing your best performing wighting respectively (i.e. raw or tfidf). (**3 marks**) **Tip: you should merge the two representations**\n",
    "\n",
    "\n",
    "\n",
    "- Binary Logistic Regression (LR) classifiers for Task 1 that will be able to accurately classify movie reviews trained with: \n",
    "    - (1) BOW-count (raw frequencies) \n",
    "    - (2) BOW-tfidf (tf.idf weighted)\n",
    "    - (3) BOCN-count\n",
    "    - (4) BOCN-tfidf\n",
    "    - (5) BOW+BOCN (best performing weighting; raw or tfidf)\n",
    "\n",
    "\n",
    "\n",
    "- Multiclass Logistic Regression classifiers for Task 2 that will be able to accurately classify news articles trained with:\n",
    "    - (1) BOW-count (raw frequencies) \n",
    "    - (2) BOW-tfidf (tf.idf weighted)\n",
    "    - (3) BOCN-count\n",
    "    - (4) BOCN-tfidf\n",
    "    - (5) BOW+BOCN (best performing weighting; raw or tfidf)\n",
    "\n",
    "\n",
    "\n",
    "- The Stochastic Gradient Descent (SGD) algorithm to estimate the parameters of your Logistic Regression models. Your SGD algorithm should:\n",
    "    - Minimise the Binary Cross-entropy loss function for Task 1 (**3 marks**)\n",
    "    - Minimise the Categorical Cross-entropy loss function for Task 2 (**3 marks**)\n",
    "    - Use L2 regularisation (**2 marks**)\n",
    "    - Perform multiple passes (epochs) over the training data (**1 mark**)\n",
    "    - Randomise the order of training data after each pass (**1 mark**)\n",
    "    - Stop training if the difference between the current and previous development loss is smaller than a threshold (**1 mark**)\n",
    "    - After each epoch print the training and development loss (**1 mark**)\n",
    "\n",
    "\n",
    "\n",
    "- Discuss how did you choose hyperparameters (e.g. learning rate and regularisation strength) for each LR model? You should use a table showing model performance using different set of hyperparameter values. (**5 marks; 2.5 for each task**). **Tip: Instead of using all possible combinations, you could perform a random sampling of combinations.**\n",
    "\n",
    "\n",
    "\n",
    "- After training each LR model, plot the learning process (i.e. training and validation loss in each epoch) using a line plot. Does your model underfit, overfit or is it about right? Explain why. (**2 marks**). \n",
    "\n",
    "\n",
    "\n",
    "- Identify and show the most important features (model interpretability) for each class (i.e. top-10 most positive and top-10 negative weights). Give the top 10 for each class and comment on whether they make sense (if they don't you might have a bug!).  If you were to apply the classifier into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain? (**3 marks; 1.5 for each task**)\n",
    "\n",
    "\n",
    "\n",
    "- Provide well documented and commented code describing all of your choices. In general, you are free to make decisions about text processing (e.g. punctuation, numbers, vocabulary size) and hyperparameter values. We expect to see justifications and discussion for all of your choices (**5 marks**). \n",
    "\n",
    "\n",
    "\n",
    "- Provide efficient solutions by using Numpy arrays when possible. Executing the whole notebook with your code should not take more than 10 minutes on a any standard computer (e.g. Intel Core i5 CPU, 8 or 16GB RAM) excluding hyperparameter tuning runs. You can find tips in [Intro to Python for NLP](https://sheffieldnlp.github.io/com6513/assets/labs/a0_python_intro.pdf) (**2 marks**). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Data - Task 1 \n",
    "\n",
    "The data you will use for Task 1 are taken from here: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and you can find it in the `./data_sentiment` folder in CSV format:\n",
    "\n",
    "- `data_sentiment/train.csv`: contains 1,400 reviews, 700 positive (label: 1) and 700 negative (label: 0) to be used for training.\n",
    "- `data_sentiment/dev.csv`: contains 200 reviews, 100 positive and 100 negative to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_sentiment/test.csv`: contains 400 reviews, 200 positive and 200 negative to be used for testing.\n",
    "\n",
    "### Data - Task 2\n",
    "\n",
    "The data you will use for Task 2 is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "- `data_topic/train.csv`: contains 2,400 news, 800 for each class to be used for training.\n",
    "- `data_topic/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_topic/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "You should submit a Jupyter Notebook file (assignment1.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex` or you can print it as PDF using your browser).\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy (excluding built-in softmax funtcions) and Pandas. You are not allowed to use any third-party library such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras etc.. You should mention if you've used Windows to write and test your code because we mostly use Unix based machines for marking (e.g. Ubuntu, MacOS). \n",
    "\n",
    "There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\\% or higher. The quality of the analysis of the results is as important as the accuracy itself. \n",
    "\n",
    "This assignment will be marked out of 40. It is worth 40\\% of your final grade in the module.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Fri, 25 Feb 2021** and it needs to be submitted via Blackboard. Standard departmental penalties for lateness will be applied. We use a range of strategies to **detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index)**, including Turnitin which helps detect plagiarism. Use of unfair means would result in getting a failing grade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T13:41:17.642162Z",
     "start_time": "2020-03-27T13:41:16.891940Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm # To show the progress\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw texts and labels into arrays\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:28.145788Z",
     "start_time": "2020-02-15T14:17:28.066100Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_train_df = pd.read_csv('./data_sentiment/train.csv',\n",
    "                                 names = ['Sentence', 'Sentiment'])\n",
    "sentiment_dev_df = pd.read_csv('./data_sentiment/dev.csv', \n",
    "                               names = ['Sentence', 'Sentiment'])\n",
    "sentiment_test_df = pd.read_csv('./data_sentiment/test.csv', \n",
    "                                names = ['Sentence', 'Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [sentiment_train_df, sentiment_dev_df, sentiment_test_df]\n",
    "sentiment_whole_doc_pd = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Pandas you can see a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:28.900892Z",
     "start_time": "2020-02-15T14:17:28.891221Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>every once in a while you see a film that is s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when i was growing up in 1970s , boys in my sc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the muppet movie is the first , and the best m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Sentiment\n",
       "0  note : some may consider portions of the follo...          1\n",
       "1  note : some may consider portions of the follo...          1\n",
       "2  every once in a while you see a film that is s...          1\n",
       "3  when i was growing up in 1970s , boys in my sc...          1\n",
       "4  the muppet movie is the first , and the best m...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to put the raw texts into Python lists and their corresponding labels into NumPy arrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.115577Z",
     "start_time": "2020-02-15T14:17:31.108038Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_train_np = sentiment_train_df.values\n",
    "sentiment_test_np = sentiment_test_df.values\n",
    "sentiment_dev_np = sentiment_dev_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Representations of Text \n",
    "\n",
    "\n",
    "To train and test Logisitc Regression models, you first need to obtain vector representations for all documents given a vocabulary of features (unigrams, bigrams, trigrams).\n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of features, you should: \n",
    "- tokenise all texts into a list of unigrams (tip: using a regular expression) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- compute bigrams, trigrams given the remaining unigrams (or character ngrams from the unigrams)\n",
    "- remove ngrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of unigrams, bigrams and trigrams (or character n-grams). You can keep top N if you encounter memory issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.860420Z",
     "start_time": "2020-02-15T14:17:31.855439Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',\n",
    "             'it', 'he', 'she', 'we', 'they' 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what', \n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "- `char_ngrams`: boolean. If true the function extracts character n-grams\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `x': a list of all extracted features.\n",
    "\n",
    "See the examples below to see how this function should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:33.169090Z",
     "start_time": "2020-02-15T14:17:33.161268Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'', \n",
    "                   stop_words=[], vocab=set(), char_ngrams=False):\n",
    "    # token_pattern=\"[a-z|\\']+\"  If char_ngrams=False\n",
    "    # token_pattern = \"[^0-9A-Za-z\\u4e00-\\u9fa5]\"  If char_ngrams=True\n",
    "    if char_ngrams == True:\n",
    "        c = \"\"\n",
    "        x = x_raw.split()\n",
    "        x = list(filter(lambda w:w not in stop_words, x)) # remove stopwords\n",
    "        x = c.join(x) \n",
    "        x = re.sub(token_pattern,'', x) # Keep letters\n",
    "    else:\n",
    "        x = re.compile(token_pattern).findall(x_raw) # only keep word in data\n",
    "        x = list(filter(lambda w:w not in stop_words, x)) # remove stopwords\n",
    "                \n",
    "    result = slide_word(x, ngram_range) # use slide_word to get ngrams\n",
    "    \n",
    "    if vocab:\n",
    "        result = list(filter(lambda d: len(set(d).intersection(vocab)) > 0, tuple(result))) \n",
    "        # get ngram which contains word in vocab\n",
    "\n",
    "    return result\n",
    "\n",
    "def slide_word(text, ngram_range):\n",
    "    result = list()\n",
    "    for n in range(ngram_range[0], ngram_range[1]+1):\n",
    "        for i in range(len(text)):\n",
    "            word = text[i:i + n]\n",
    "            if len(word) < n:\n",
    "                break\n",
    "            result.append(tuple(word))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary \n",
    "\n",
    "The `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n",
    "\n",
    "Hint: it should make use of the `extract_ngrams` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:35.821240Z",
     "start_time": "2020-02-15T14:17:35.814722Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'', \n",
    "              min_df=0, keep_topN=0, \n",
    "              stop_words=[],char_ngrams=False):\n",
    "    \n",
    "    x_array = []\n",
    "    df_all = {}\n",
    "    n = len(X_raw)\n",
    "    for comment in tqdm(X_raw):\n",
    "        x = extract_ngrams(comment, ngram_range=ngram_range, token_pattern=token_pattern,\n",
    "                          stop_words=stop_words, char_ngrams=char_ngrams) \n",
    "        # get ngrams for each comment\n",
    "        x_array.extend(x)\n",
    "        \n",
    "        x_set = set(x)\n",
    "        for word in x_set:\n",
    "            df_all[word] = df_all.get(word, 0) + 1 \n",
    "            # Calculate how many documents contains ngrams\n",
    "            \n",
    "    df_min = {k:v for k,v in df_all.items() if v > min_df} \n",
    "    # Get the df_min dictionary that contains those ngram which appear more than min_df times\n",
    "            \n",
    "    ngram_counts_all = Counter(x_array)\n",
    "    ngram_counts_sorted = sorted(ngram_counts_all.items(),key=lambda item:item[1],reverse=True)\n",
    "    # Sort ngrams to help calculate topN\n",
    "    \n",
    "    if keep_topN == 0:\n",
    "        ngram_counts_top = list(dict(ngram_counts_sorted).keys())#return all\n",
    "    elif keep_topN < 1:\n",
    "        ngram_counts_top = list(dict(ngram_counts_sorted).keys())[:int(len(ngram_counts_sorted)*keep_topN)] \n",
    "        #return topN by percentages\n",
    "    else:\n",
    "        ngram_counts_top =  list(dict(ngram_counts_sorted).keys())[:keep_topN] \n",
    "        #return first keep_topN ngram\n",
    "\n",
    "    vocab = set(df_min.keys()).intersection(set(ngram_counts_top))\n",
    "    #select the intersection of min_df and keep_topN\n",
    "    df ={}\n",
    "    ngram_counts ={}\n",
    "    for word in vocab:\n",
    "        df[word] = df_all[word]\n",
    "        ngram_counts[word] = ngram_counts_all[word]\n",
    "    \n",
    "    \n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.319793Z",
     "start_time": "2020-02-15T14:17:36.836545Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1400/1400 [00:02<00:00, 649.88it/s]\n"
     ]
    }
   ],
   "source": [
    "sentiment_train_vocab, sentiment_train_df, sentiment_train_ngram_counts = get_vocab(sentiment_train_np[:,0], \n",
    "                                                                                    token_pattern=\"[a-z|\\']+\", \n",
    "                                                                                    stop_words=stop_words, min_df=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create 2 dictionaries: (1) vocabulary id -> word; and  (2) word -> vocabulary id so you can use them for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.326811Z",
     "start_time": "2020-02-15T14:17:39.322256Z"
    }
   },
   "outputs": [],
   "source": [
    "id2word = dict(enumerate(sentiment_train_vocab))\n",
    "word2id = dict(zip(sentiment_train_vocab, range(len(sentiment_train_vocab))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to extract n-grams for each text in the training, development and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_extracy_ngrams(documents, char_ngrams = False, token_pattern = r''):\n",
    "    result = {}\n",
    "    i = 0\n",
    "    for comment in tqdm(documents):\n",
    "        result[i] = extract_ngrams(comment, token_pattern=token_pattern, stop_words=stop_words, char_ngrams=char_ngrams)\n",
    "        i += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1400/1400 [00:01<00:00, 1074.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 1165.90it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1132.97it/s]\n"
     ]
    }
   ],
   "source": [
    "sentiment_train_extract_ngrams = whole_extracy_ngrams(sentiment_train_np[:,0], token_pattern=\"[a-z|\\']+\")\n",
    "sentiment_test_extrace_ngrams = whole_extracy_ngrams(sentiment_test_np[:,0], token_pattern=\"[a-z|\\']+\")\n",
    "sentiment_dev_extrace_ngrams = whole_extracy_ngrams(sentiment_dev_np[:,0], token_pattern=\"[a-z|\\']+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:\n",
    "- `X_ngram`: a list of texts (documents), where each text is represented as list of n-grams in the `vocab`\n",
    "- `vocab`: a set of n-grams to be used for representing the documents\n",
    "\n",
    "and return:\n",
    "- `X_vec`: an array with dimensionality Nx|vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:40.219201Z",
     "start_time": "2020-02-15T14:17:40.215129Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorise(X_ngram, vocab):\n",
    "    N = len(X_ngram)\n",
    "    size_vocab = len(vocab)\n",
    "    X_vec = np.zeros(shape=(N, size_vocab))\n",
    "    for index, n_grams in tqdm(X_ngram.items()):\n",
    "        frequency = Counter(n_grams)\n",
    "        j = 0\n",
    "        for word in vocab:\n",
    "            X_vec[index,j] = frequency.get(word,0)\n",
    "            j += 1\n",
    "    return X_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `vectorise` to obtain document vectors for each document in the train, development and test set. You should extract both count and tf.idf vectors respectively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:41.999574Z",
     "start_time": "2020-02-15T14:17:40.376534Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1400/1400 [00:10<00:00, 138.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 400/400 [00:03<00:00, 130.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 131.76it/s]\n"
     ]
    }
   ],
   "source": [
    "sentiment_train_tfmatrix = vectorise(sentiment_train_extract_ngrams, sentiment_train_vocab)\n",
    "sentiment_test_tfmatrix = vectorise(sentiment_test_extrace_ngrams, sentiment_train_vocab)\n",
    "sentiment_dev_tfmatrix = vectorise(sentiment_dev_extrace_ngrams, sentiment_train_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.IDF vectors\n",
    "\n",
    "First compute `idfs` an array containing inverted document frequencies (Note: its elements should correspond to your `vocab`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:42.022692Z",
     "start_time": "2020-02-15T14:17:42.012315Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentiment_idf = []\n",
    "number_documents = len(sentiment_train_np)\n",
    "for word in sentiment_train_vocab:\n",
    "    df = sentiment_train_df.get(word)\n",
    "    sentiment_idf.append(np.log(number_documents/df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then transform your count vectors to tf.idf vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_matrix = sentiment_train_tfmatrix * np.array(sentiment_idf)\n",
    "tfidf_dev_matrix = sentiment_dev_tfmatrix * np.array(sentiment_idf)\n",
    "tfidf_test_matrix = sentiment_test_tfmatrix * np.array(sentiment_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "After obtaining vector representations of the data, now you are ready to implement Binary Logistic Regression for classifying sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement the `sigmoid` function. It takes as input:\n",
    "\n",
    "- `z`: a real number or an array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `sig`: the sigmoid of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.160661Z",
     "start_time": "2020-02-15T14:17:44.157902Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sig = 1 / (1 + np.exp(-z))\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_proba` function to obtain prediction probabilities. It takes as input:\n",
    "\n",
    "- `X`: an array of inputs, i.e. documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_proba`: the prediction probabilities of X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.718566Z",
     "start_time": "2020-02-15T14:17:44.715017Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    \n",
    "    preds_proba = sigmoid(np.dot(X, weights))\n",
    "    \n",
    "    return preds_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_class` function to obtain the most probable class for each vector in an array of input vectors. It takes as input:\n",
    "\n",
    "- `X`: an array of documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_class`: the predicted class for each x in X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.002125Z",
     "start_time": "2020-02-15T14:17:44.998668Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    \n",
    "    z = predict_proba(X, weights)\n",
    "    preds_class = np.where(z>0.5,1,0)\n",
    "    \n",
    "    return preds_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the weights from data, we need to minimise the binary cross-entropy loss. Implement `binary_loss` that takes as input:\n",
    "\n",
    "- `X`: input vectors\n",
    "- `Y`: labels\n",
    "- `weights`: model weights\n",
    "- `alpha`: regularisation strength\n",
    "\n",
    "and return:\n",
    "\n",
    "- `l`: the loss score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.455533Z",
     "start_time": "2020-02-15T14:17:45.451475Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_loss(X, Y, weights, alpha=0.00001):\n",
    "    '''\n",
    "    Binary Cross-entropy Loss\n",
    "\n",
    "    X:(len(X),len(vocab))\n",
    "    Y: array len(Y)\n",
    "    weights: array len(X)\n",
    "    '''\n",
    "    preds_proba = predict_proba(X, weights)\n",
    "    preds_proba = np.clip(preds_proba,1e-9,1-1e-9)\n",
    "    l = -np.mean(Y*np.log(preds_proba)+(1-Y)*np.log(1-preds_proba)) + alpha * (weights.T @ weights)\n",
    "    #L2 regularisation\n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can implement Stochastic Gradient Descent to learn the weights of your sentiment classifier. The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `alpha`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.968510Z",
     "start_time": "2020-02-15T14:17:45.958185Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev=[], Y_dev=[], lr=0.1, \n",
    "        alpha=1e-5, epochs=5, \n",
    "        tolerance=0.0001, print_progress=True):\n",
    "        X_tr = np.array(X_tr)\n",
    "        Y_tr = np.array(Y_tr)\n",
    "        X_dev = np.array(X_dev)\n",
    "        Y_dev = np.array(Y_dev)\n",
    "        m, n = X_tr.shape\n",
    "        weights = np.random.normal(0,0.1,n)\n",
    "        epochs_count = 0\n",
    "        training_loss_history = []\n",
    "        validation_loss_history = []\n",
    "        epochs_list = []\n",
    "        index = list(range(m))\n",
    "        validation_loss = np.inf\n",
    "        training_loss = np.inf\n",
    "        b = 0\n",
    "        \n",
    "        for epochs_count in tqdm(range(epochs)): # Multiple passes\n",
    "            np.random.shuffle(index) # Randomise the order of training data after each pass\n",
    "            for i in index:\n",
    "                hypothesis = predict_proba(X_tr[i], weights)\n",
    "                error = Y_tr[i] - hypothesis\n",
    "                grad = -np.array(np.dot(error,X_tr[i]).tolist()) + 2 * weights * alpha/len(weights) \n",
    "                #Gradient with L2 regularisation\n",
    "                #Minimise the Binary Cross-entropy loss function\n",
    "                weights = weights - lr * grad\n",
    "                \n",
    "            training_loss_new = binary_loss(X_tr, Y_tr, weights=weights, alpha=alpha) \n",
    "            # Training loss\n",
    "            training_loss_history.append(training_loss_new)\n",
    "\n",
    "            validation_loss_new = binary_loss(X_dev, Y_dev, weights=weights, alpha=alpha) \n",
    "            #Validation loss\n",
    "            validation_loss_history.append(validation_loss_new)\n",
    "            if print_progress==True: # Print the training and developemnt loss\n",
    "                print('Epoch:{}   training_loss:{}   validation_loss:{}'.format(epochs_count,training_loss_new,validation_loss_new))\n",
    "            if abs(validation_loss_new - validation_loss) < tolerance: \n",
    "                # Stop training if the difference smaller than a threshold more than b times\n",
    "                b += 1\n",
    "                if b > 0:\n",
    "                    break\n",
    "            else:\n",
    "                b = 0\n",
    "                validation_loss = validation_loss_new\n",
    "                training_loss = training_loss_new\n",
    "            epochs_list.append(epochs_count)\n",
    "        print(\"Stop Epoch: {}\".format(epochs_count))\n",
    "        return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with Count vectors\n",
    "\n",
    "First train the model using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████▏                                                            | 8/30 [00:17<00:47,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Bow_count, Bow_training_loss_history, Bow_validation_loss_history = SGD(sentiment_train_tfmatrix, sentiment_train_np[:,1], \n",
    "                                                                        sentiment_dev_tfmatrix, sentiment_dev_np[:,1],\n",
    "                                                                        lr=0.06,epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch for the best hyperparameter combination. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:52:26.583150Z",
     "start_time": "2020-01-21T16:52:26.578754Z"
    }
   },
   "source": [
    "The model is about right. The figure showed that the training loss and validation loss has decreased in the first few epochs and becomes stable after several epochs which means the model did not underfit. At the end of training, the loss did not vibrate or increase which means the model isn't overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x288c8d4c438>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Bow_training_loss_history, label = \"Training\")\n",
    "plt.plot(Bow_validation_loss_history, label = \"Validation\")\n",
    "plt.title(\"Loss-Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8175\n",
      "Precision: 0.7953488372093023\n",
      "Recall: 0.855\n",
      "F1-Score: 0.8240963855421686\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(sentiment_test_tfmatrix)\n",
    "Y_te = np.array(sentiment_test_np[:,1].tolist())\n",
    "preds_te_count = predict_class(X_te_count, Bow_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print the top-10 words for the negative and positive class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.613935Z",
     "start_time": "2020-02-15T14:17:51.610660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bad',)\n",
      "('unfortunately',)\n",
      "('worst',)\n",
      "('script',)\n",
      "('any',)\n",
      "('boring',)\n",
      "('plot',)\n",
      "('west',)\n",
      "('why',)\n",
      "('silly',)\n"
     ]
    }
   ],
   "source": [
    "top_neg = Bow_count.argsort()[:10]\n",
    "for i in top_neg:\n",
    "    print(id2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.624122Z",
     "start_time": "2020-02-15T14:17:51.615674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('great',)\n",
      "('movies',)\n",
      "('without',)\n",
      "('works',)\n",
      "('cauldron',)\n",
      "('quite',)\n",
      "('perfectly',)\n",
      "('hilarious',)\n",
      "('see',)\n",
      "('life',)\n"
     ]
    }
   ],
   "source": [
    "top_pos = Bow_count.argsort()[::-1][:10]\n",
    "for i in top_pos:\n",
    "    print(id2word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, the classifier we've learned may not have good performance when it applied to a new domain. The classifier with the parameters was learned on the sentiment analysis dataset. The weights of the classifier should corresponding to the features which means when we apply the classfier to a different domain, the features of the new domain will same as the sentiment dataset. In fact, some words which have high frequcy in the sentiment dataset may not appear in the different domain's dataset and it will affect the results of the classifier.\n",
    "\n",
    "Those features with obvious emotion in the different domain could be picked up as important. If the classifier be used to do sentiment analysis in a different domain, the words with obvious emotion will not have too much difference with those words in sentiment train dataset which means the weights learned from the sentiment train dataset can also be applied to those fearures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example1: lr=5 ,alpha=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]D:\\PROGRAM\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:55<00:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "E1_count, E1_training_loss_history, E1_validation_loss_history = SGD(sentiment_train_tfmatrix, sentiment_train_np[:,1], sentiment_dev_tfmatrix, sentiment_dev_np[:,1],\n",
    "                                                                        lr=10,epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8175\n",
      "Precision: 0.7981220657276995\n",
      "Recall: 0.85\n",
      "F1-Score: 0.8232445520581112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PROGRAM\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(sentiment_test_tfmatrix)\n",
    "Y_te = np.array(sentiment_test_np[:,1].tolist())\n",
    "preds_te_count = predict_class(X_te_count, E1_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example2: lr=1e-5 , alpha=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:55<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "E2_count, E2_training_loss_history, E2_validation_loss_history = SGD(sentiment_train_tfmatrix, sentiment_train_np[:,1], sentiment_dev_tfmatrix, sentiment_dev_np[:,1],\n",
    "                                                                        lr=1e-5,epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5425\n",
      "Precision: 0.5391705069124424\n",
      "Recall: 0.585\n",
      "F1-Score: 0.5611510791366906\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(sentiment_test_tfmatrix)\n",
    "Y_te = np.array(sentiment_test_np[:,1].tolist())\n",
    "preds_te_count = predict_class(X_te_count, E2_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example3: alpha=1e-9, lr=0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████▏                                                            | 8/30 [00:16<00:45,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "E3_count, E3_training_loss_history, E3_validation_loss_history = SGD(sentiment_train_tfmatrix, sentiment_train_np[:,1], \n",
    "                                                                     sentiment_dev_tfmatrix, sentiment_dev_np[:,1],\n",
    "                                                                    lr=0.06, alpha=1e-9,epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8425\n",
      "Precision: 0.8341463414634146\n",
      "Recall: 0.855\n",
      "F1-Score: 0.8444444444444444\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(sentiment_test_tfmatrix)\n",
    "Y_te = np.array(sentiment_test_np[:,1].tolist())\n",
    "preds_te_count = predict_class(X_te_count, E3_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example3: alpha=1, lr=0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:55<00:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "E4_count, E4_training_loss_history, E4_validation_loss_history = SGD(sentiment_train_tfmatrix, sentiment_train_np[:,1], \n",
    "                                                                     sentiment_dev_tfmatrix, sentiment_dev_np[:,1],\n",
    "                                                                    lr=0.06, alpha=1,epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8225\n",
      "Precision: 0.8\n",
      "Recall: 0.86\n",
      "F1-Score: 0.8289156626506025\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(sentiment_test_tfmatrix)\n",
    "Y_te = np.array(sentiment_test_np[:,1].tolist())\n",
    "preds_te_count = predict_class(X_te_count, E4_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select the appropriate parameters, we need to use different sets of hyperparameters. To show the different results of different examples, parameters with huge differences will be used. Also, the difference in parameters within ten times of the appropriate has little effect on the results. We can notice that some example may achieve a higher F1-socre that we used, but they also need more epochs to finish training but only increase the F1-Score less than 0.01 which will waste our time.\n",
    "\n",
    "When the learning rate is too large or too small, the classifier will need more epochs to finish training. Also, when the learning rate is too large, SGD may difficult to get reach local optimum and will vibrate around the local optimum so that it will need more epochs. When the learning weight is too small, the gradiend will converge with a small convergence rate and it will also need more epochs to reach the local optimum.\n",
    "\n",
    "L2 regularisation aims to give a \"penaty\" to the loss. A small regularisation strength may cause underfit and a big regularisation stregnth may cause overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Example | Learing Rate  | Alpha | Epochs | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  | 0.06  | 1e-5  | 8  | 0.824|\n",
    "| Example1  | 10  | 1e-5  | >30  | 0.823|\n",
    "| Example2  | 1e-5  |  1e-5 | >30 | 0.561|\n",
    "| Example3  | 0.06  |  1e-9 | >30 |0.844|\n",
    "| Example4  | 0.06  |  1 | >30   | 0.829|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with TF.IDF vectors\n",
    "\n",
    "Follow the same steps as above (i.e. evaluating count n-gram representations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]D:\\PROGRAM\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      " 23%|███████████████████▎                                                               | 7/30 [00:15<00:50,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w_count, training_loss_history, validation_loss_history = SGD(tfidf_train_matrix,sentiment_train_np[:,1], tfidf_dev_matrix, \n",
    "                                                              sentiment_dev_np[:,1],lr=0.06,epochs=30,tolerance=0.001,\n",
    "                                                              print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x288f4e91550>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss_history, label = \"Training\")\n",
    "plt.plot(validation_loss_history, label = \"Validation\")\n",
    "plt.title(\"Loss-Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.805\n",
      "Precision: 0.7699115044247787\n",
      "Recall: 0.87\n",
      "F1-Score: 0.8169014084507041\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(tfidf_test_matrix)\n",
    "Y_te = np.array(sentiment_test_np[:,1].tolist())\n",
    "preds_te_count = predict_class(X_te_count, w_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dune',)\n",
      "('hammond',)\n",
      "('unfortunately',)\n",
      "('bad',)\n",
      "('stahl',)\n",
      "('spawn',)\n",
      "('filmmakers',)\n",
      "('data',)\n",
      "('ridiculous',)\n",
      "('spice',)\n"
     ]
    }
   ],
   "source": [
    "top_neg = w_count.argsort()[:10]\n",
    "for i in top_neg:\n",
    "    print(id2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('exotica',)\n",
      "('shaw',)\n",
      "('toys',)\n",
      "('lebowski',)\n",
      "('trek',)\n",
      "('sweet',)\n",
      "('ellie',)\n",
      "('spacey',)\n",
      "('movement',)\n",
      "('raimi',)\n"
     ]
    }
   ],
   "source": [
    "top_pos = w_count.argsort()[::-1][:10]\n",
    "for i in top_pos:\n",
    "    print(id2word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now repeat the training and evaluation process for BOW-tfidf, BOCN-count, BOCN-tfidf, BOW+BOCN including hyperparameter tuning for each model...\n",
    "#### BOW-tfidf has showed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOCN-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1400/1400 [00:05<00:00, 236.36it/s]\n"
     ]
    }
   ],
   "source": [
    "sentiment_trainbocn_vocab, sentiment_trainbocn_df, sentiment_trainbocn_counts = get_vocab(sentiment_train_np[:,0],\n",
    "                                                                                          token_pattern = \"[^0-9A-Za-z\\u4e00-\\u9fa5]\", \n",
    "                                                                                          stop_words=stop_words, min_df=5,\n",
    "                                                                                          char_ngrams=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1400/1400 [00:04<00:00, 319.25it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 329.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 318.31it/s]\n"
     ]
    }
   ],
   "source": [
    "sentiment_trainbocn_extract_ngrams = whole_extracy_ngrams(sentiment_train_np[:,0], char_ngrams=True, \n",
    "                                                          token_pattern = \"[^0-9A-Za-z\\u4e00-\\u9fa5]\")\n",
    "sentiment_testbocn_extrace_ngrams = whole_extracy_ngrams(sentiment_test_np[:,0], char_ngrams=True, \n",
    "                                                         token_pattern = \"[^0-9A-Za-z\\u4e00-\\u9fa5]\")\n",
    "sentiment_devbocn_extrace_ngrams = whole_extracy_ngrams(sentiment_dev_np[:,0], char_ngrams=True, \n",
    "                                                        token_pattern = \"[^0-9A-Za-z\\u4e00-\\u9fa5]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1400/1400 [00:04<00:00, 344.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 335.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 367.28it/s]\n"
     ]
    }
   ],
   "source": [
    "sentiment_trainbocn_tfmatrix = vectorise(sentiment_trainbocn_extract_ngrams, sentiment_trainbocn_vocab)\n",
    "sentiment_testbocn_tfmatrix = vectorise(sentiment_testbocn_extrace_ngrams, sentiment_trainbocn_vocab)\n",
    "sentiment_devbocn_tfmatrix = vectorise(sentiment_devbocn_extrace_ngrams, sentiment_trainbocn_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]D:\\PROGRAM\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:21<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Bocn_count, Bocn_training_loss_history, Bocn_validation_loss_history = SGD(sentiment_trainbocn_tfmatrix, sentiment_train_np[:,1], \n",
    "                                                                        sentiment_devbocn_tfmatrix, sentiment_dev_np[:,1],\n",
    "                                                                        lr=0.06,epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n",
      "Precision: 0.78\n",
      "Recall: 0.78\n",
      "F1-Score: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PROGRAM\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(sentiment_testbocn_tfmatrix)\n",
    "Y_te = np.array(sentiment_test_np[:,1].tolist())\n",
    "preds_te_count = predict_class(X_te_count, Bocn_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bocn-tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_bocnidf = []\n",
    "for word in sentiment_trainbocn_vocab:\n",
    "    df = sentiment_trainbocn_df.get(word)\n",
    "    sentiment_bocnidf.append(np.log(number_documents/df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bocntfidf_train_matrix = sentiment_trainbocn_tfmatrix * sentiment_bocnidf\n",
    "bocntfidf_dev_matrix = sentiment_devbocn_tfmatrix * sentiment_bocnidf\n",
    "bocntfidf_test_matrix = sentiment_testbocn_tfmatrix * sentiment_bocnidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]D:\\PROGRAM\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      " 33%|███████████████████████████▎                                                      | 10/30 [00:08<00:16,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Bocntfidf_count,Bocntfidf_training_loss_history,Bocntfidf_validation_loss_history = SGD(bocntfidf_train_matrix, sentiment_train_np[:,1], \n",
    "                                                                        bocntfidf_dev_matrix, sentiment_dev_np[:,1],\n",
    "                                                                        lr=0.06,epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7875\n",
      "Precision: 0.7860696517412935\n",
      "Recall: 0.79\n",
      "F1-Score: 0.7880299251870324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PROGRAM\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(bocntfidf_test_matrix)\n",
    "Y_te = np.array(sentiment_test_np[:,1].tolist())\n",
    "preds_te_count = predict_class(X_te_count, Bocntfidf_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW(tfidf)+BOCN(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_bocn_train = np.hstack((tfidf_train_matrix,bocntfidf_train_matrix))\n",
    "bow_bocn_dev = np.hstack((tfidf_dev_matrix, bocntfidf_dev_matrix))\n",
    "bow_bocn_test = np.hstack((tfidf_test_matrix, bocntfidf_test_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]D:\\PROGRAM\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      " 20%|████████████████▌                                                                  | 6/30 [00:18<01:15,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bowbocn_count, bowbocn_training_loss_history, bowbocn_validation_loss_history = SGD(bow_bocn_train, sentiment_train_np[:,1], \n",
    "                                                                        bow_bocn_dev, sentiment_dev_np[:,1],\n",
    "                                                                        lr=0.06,epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8225\n",
      "Precision: 0.8056872037914692\n",
      "Recall: 0.85\n",
      "F1-Score: 0.8272506082725061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PROGRAM\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(bow_bocn_test)\n",
    "Y_te = np.array(sentiment_test_np[:,1].tolist())\n",
    "preds_te_count = predict_class(X_te_count, bowbocn_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  | 0.795  | 0.855  | 0.824  |\n",
    "| BOW-tfidf  | 0.805  | 0.770 | 0.817  |\n",
    "| BOCN-count  | 0.78  | 0.78  | 0.78  |\n",
    "| BOCN-tfidf  | 0.786  | 0.79  | 0.788  |\n",
    "| BOW(tfidf)+BOCN(tfidf)  |0.806  | 0.85  | 0.827  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is BOW(tfidf)+BOCN(tfidf), it combines BOW-tfidf and BOCN-tfidf which can have more features and representative the dataset better. Also, based on the equation: tfidf=TF*IDF , IDF can reduce the importance of words that appear more frequency in the document in the model which can help the classifier give higher weights to those words which can representitive the sentiment of sentence better.\n",
    "\n",
    "Also, we can notice that BOCN-count has the worst performance in the five models. It might because of the ngram range is (1,3) which means features will contain some single letters which will appear in all sentence with a high weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Multi-class Logistic Regression \n",
    "\n",
    "Now you need to train a Multiclass Logistic Regression (MLR) Classifier by extending the Binary model you developed above. You will use the MLR model to perform topic classification on the AG news dataset consisting of three classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class 1: World\n",
    "- Class 2: Sports\n",
    "- Class 3: Business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to follow the same process as in Task 1 for data processing and feature extraction by reusing the functions you wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:03.212229Z",
     "start_time": "2020-02-15T14:18:03.185261Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_train_df = pd.read_csv('./data_topic/train.csv',names = ['Sentence', 'Sentiment'])\n",
    "topic_test_df = pd.read_csv('./data_topic/test.csv',names = ['Sentence', 'Sentiment'])\n",
    "topic_dev_df = pd.read_csv('./data_topic/dev.csv',names = ['Sentence', 'Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BAGHDAD, Iraq - An Islamic militant group that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Parts of Los Angeles international airport are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>AFP - Facing a issue that once tripped up his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The leader of militant Lebanese group Hezbolla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>JAKARTA : ASEAN finance ministers ended a meet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence                                          Sentiment\n",
       "0         1  BAGHDAD, Iraq - An Islamic militant group that...\n",
       "1         1  Parts of Los Angeles international airport are...\n",
       "2         1  AFP - Facing a issue that once tripped up his ...\n",
       "3         1  The leader of militant Lebanese group Hezbolla...\n",
       "4         1  JAKARTA : ASEAN finance ministers ended a meet..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_train_np = topic_train_df.values\n",
    "topic_test_np = topic_test_df.values\n",
    "topic_dev_np = topic_dev_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_train_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to change `SGD` to support multiclass datasets. First you need to develop a `softmax` function. It takes as input:\n",
    "\n",
    "- `z`: array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `smax`: the softmax of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:07.440998Z",
     "start_time": "2020-02-15T14:18:07.437915Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    smax = np.longfloat(np.exp(z - np.max(z)))\n",
    "    smax = smax/np.sum(smax).reshape(-1,1)\n",
    "    return smax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then modify `predict_proba` and `predict_class` functions for the multiclass case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:07.445451Z",
     "start_time": "2020-02-15T14:18:07.442851Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    \n",
    "    preds_proba = softmax(np.dot(X, weights.T))\n",
    "                          \n",
    "    return preds_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:07.449814Z",
     "start_time": "2020-02-15T14:18:07.447145Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    z = predict_proba(X, weights)\n",
    "    z_list = z.tolist()\n",
    "    preds_class = np.argmax(z_list, axis=1).reshape(-1,1) + np.ones(shape=(len(z_list),1))\n",
    "    return preds_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to compute the categorical cross entropy loss (extending the binary loss to support multiple classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:09.095415Z",
     "start_time": "2020-02-15T14:18:09.090680Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_loss(X, Y, weights, num_classes=5, alpha=0.00001):\n",
    "    '''\n",
    "    X:(len(X),len(vocab))\n",
    "    Y: array len(Y)\n",
    "    weights: array len(X)\n",
    "    '''\n",
    "    preds_prob = np.clip(predict_proba(X, weights),1e-6, 1-1e-6)\n",
    "    l = np.sum(-np.log(np.max(preds_prob, axis=1))) + alpha * np.sum(weights * weights)/(num_classes * len(X))\n",
    "    # Categorical loss with L2 regularisation\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support the categorical cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:10.176885Z",
     "start_time": "2020-02-15T14:18:10.165021Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev=[], Y_dev=[], num_classes=5, lr=0.01, alpha=0.00001, \n",
    "        epochs=5, tolerance=0.001, print_progress=True):\n",
    "        X_tr = np.array(X_tr)\n",
    "        Y_tr = np.array(Y_tr)\n",
    "        X_dev = np.array(X_dev)\n",
    "        Y_dev = np.array(Y_dev)\n",
    "        m, n = X_tr.shape\n",
    "        weights = np.random.randn(num_classes, n) # Initialize weights\n",
    "        epochs_count = 0\n",
    "        training_loss_history = []\n",
    "        validation_loss_history = []\n",
    "        epochs_list = []\n",
    "        index = list(range(m))\n",
    "        validation_loss = np.inf\n",
    "        training_loss = np.inf\n",
    "        b = 0\n",
    "        \n",
    "        for epochs_count in tqdm(range(epochs)): #Perform multiple passes (epochs) over the training data\n",
    "            np.random.shuffle(index) # Randomise the order of training data after each pass \n",
    "            for i in index:\n",
    "                hypothesis = predict_proba(X_tr[i], weights)\n",
    "                y = np.zeros((1,num_classes))\n",
    "                y[0,Y_tr[i]-1] = 1\n",
    "                grad = np.array((hypothesis-y).T * X_tr[i].reshape(-1,1).T) + weights * alpha/ (num_classes*n)\n",
    "                #Use L2 regularisation\n",
    "                weights = weights - lr * grad\n",
    "                #Minimise the Categorical Cross-entropy loss function \n",
    "                \n",
    "            training_loss_new = np.sum(categorical_loss(X_tr, Y_tr, weights, alpha=alpha, num_classes=num_classes))/ m # training loss\n",
    "            training_loss_history.append(training_loss_new)\n",
    "\n",
    "            validation_loss_new = np.sum(categorical_loss(X_dev, \n",
    "                                                          Y_dev, weights, alpha=alpha, num_classes=num_classes)) / X_dev.shape[0] \n",
    "            #Loss after\n",
    "            validation_loss_history.append(validation_loss_new)\n",
    "            if print_progress==True:\n",
    "                print('Epoch:{}   training_loss:{}   validation_loss:{}'.format(epochs_count,training_loss_new,validation_loss_new))\n",
    "            if abs(validation_loss_new - validation_loss) < tolerance: \n",
    "                #Stop training if the difference between the current and previous development loss is smaller than a threshold\n",
    "                b += 1\n",
    "                if b > 1: # b representitive how many times that the difference of loss is smaller than threshold\n",
    "                    break\n",
    "            else:\n",
    "                b = 0\n",
    "                validation_loss = validation_loss_new\n",
    "                training_loss = training_loss_new\n",
    "            epochs_list.append(epochs_count)\n",
    "        print(\"Stop Epoch: {}\".format(epochs_count))    \n",
    "        return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate you MLR following the same steps as in Task 1 for the different vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:55.324956Z",
     "start_time": "2020-02-15T14:18:11.720952Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2400/2400 [00:00<00:00, 13295.18it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_train_vocab, topic_train_df, topic_train_counts = get_vocab(topic_train_np[:,1],token_pattern=\"[a-z|\\']+\",\n",
    "                                                                           stop_words=stop_words, min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2400/2400 [00:00<00:00, 20053.65it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 900/900 [00:00<00:00, 21486.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 21485.01it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_train_extract_ngrams = whole_extracy_ngrams(topic_train_np[:,1], token_pattern=\"[a-z|\\']+\")\n",
    "topic_test_extract_ngrams = whole_extracy_ngrams(topic_test_np[:,1], token_pattern=\"[a-z|\\']+\")\n",
    "topic_dev_extract_ngrams = whole_extracy_ngrams(topic_dev_np[:,1], token_pattern=\"[a-z|\\']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2400/2400 [00:01<00:00, 1555.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 900/900 [00:00<00:00, 1611.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 1600.02it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_train_tfmatrix = vectorise(topic_train_extract_ngrams, topic_train_vocab)\n",
    "topic_test_tfmatrix = vectorise(topic_test_extract_ngrams, topic_train_vocab)\n",
    "topic_dev_tfmatrix = vectorise(topic_dev_extract_ngrams, topic_train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:05<00:00,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Bow_count, Bow_training_loss_history, Bow_validation_loss_history = SGD(topic_train_tfmatrix, topic_train_np[:,0], topic_dev_tfmatrix, topic_dev_np[:,0],\n",
    "                                                                        num_classes=3,lr=0.06,epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:20:54.864071Z",
     "start_time": "2020-02-15T14:20:54.850199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7811111111111111\n",
      "Precision: 0.7830223923945304\n",
      "Recall: 0.7811111111111111\n",
      "F1-Score: 0.7812287281881019\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(topic_test_tfmatrix)\n",
    "label = np.array(topic_test_np[:,0])\n",
    "predicted = predict_class(X_te_count, Bow_count)\n",
    "preds_te = list()\n",
    "Y_te = list()\n",
    "\n",
    "for i in range(len(label)):\n",
    "    Y_te.append(int(label[i]))\n",
    "    preds_te.append(int(predicted[i]))\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example1: lr=5 ,alpha=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▌                                                                             | 2/30 [00:00<00:06,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "E1_count, E1_training_loss_history, E1_validation_loss_history = SGD(topic_train_tfmatrix, topic_train_np[:,0], \n",
    "                                                                     topic_dev_tfmatrix, topic_dev_np[:,0],\n",
    "                                                                     num_classes=3,lr=5, alpha=1e-5,\n",
    "                                                                     epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333333333333333\n",
      "Precision: 0.24575610039906803\n",
      "Recall: 0.3333333333333333\n",
      "F1-Score: 0.2026547227529373\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(topic_train_tfmatrix)\n",
    "label = np.array(topic_test_np[:,0])\n",
    "predicted = predict_class(X_te_count, E1_count)\n",
    "preds_te = list()\n",
    "Y_te = list()\n",
    "\n",
    "for i in range(len(label)):\n",
    "    Y_te.append(int(label[i]))\n",
    "    preds_te.append(int(predicted[i]))\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example2: lr=1e-5 ,alpha=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:05<00:00,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "E2_count, E2_training_loss_history, E2_validation_loss_history = SGD(topic_train_tfmatrix, topic_train_np[:,0], \n",
    "                                                                     topic_dev_tfmatrix, topic_dev_np[:,0],\n",
    "                                                                    num_classes=3,lr=1e-5,alpha=1e-5,epochs=30,\n",
    "                                                                     tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34\n",
      "Precision: 0.3385446022977901\n",
      "Recall: 0.34\n",
      "F1-Score: 0.338886941947756\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(topic_train_tfmatrix)\n",
    "label = np.array(topic_test_np[:,0])\n",
    "predicted = predict_class(X_te_count, E2_count)\n",
    "preds_te = list()\n",
    "Y_te = list()\n",
    "\n",
    "for i in range(len(label)):\n",
    "    Y_te.append(int(label[i]))\n",
    "    preds_te.append(int(predicted[i]))\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example3: lr=0.04 ,alpha=1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▌                                                                             | 2/30 [00:00<00:08,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "E3_count, E3_training_loss_history, E3_validation_loss_history = SGD(topic_train_tfmatrix, topic_train_np[:,0], \n",
    "                                                                     topic_dev_tfmatrix, topic_dev_np[:,0],\n",
    "                                                                    lr=0.06, alpha=1e-9,epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33\n",
      "Precision: 0.18050706413952444\n",
      "Recall: 0.198\n",
      "F1-Score: 0.1377288796023673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PROGRAM\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(topic_train_tfmatrix)\n",
    "label = np.array(topic_test_np[:,0])\n",
    "predicted = predict_class(X_te_count, E3_count)\n",
    "preds_te = list()\n",
    "Y_te = list()\n",
    "\n",
    "for i in range(len(label)):\n",
    "    Y_te.append(int(label[i]))\n",
    "    preds_te.append(int(predicted[i]))\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example4: lr=0.04 ,alpha=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:06<00:00,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "E4_count, E4_training_loss_history, E4_validation_loss_history = SGD(topic_train_tfmatrix, topic_train_np[:,0], topic_dev_tfmatrix, topic_dev_np[:,0],\n",
    "                                                                        lr=0.06, alpha=1,epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33111111111111113\n",
      "Precision: 0.12479061976549415\n",
      "Recall: 0.3311111111111111\n",
      "F1-Score: 0.1812652068126521\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(topic_train_tfmatrix)\n",
    "label = np.array(topic_test_np[:,0])\n",
    "predicted = predict_class(X_te_count, E4_count)\n",
    "preds_te = list()\n",
    "Y_te = list()\n",
    "\n",
    "for i in range(len(label)):\n",
    "    Y_te.append(int(label[i]))\n",
    "    preds_te.append(int(predicted[i]))\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we notice that the way to select hyperparameters is same with task1 and the difference in parameters within ten times of the appropriate has little effect on the results. 4 examples has showed above and it is easily to find that those hyperparameters in examples can not increase the perofrmance of the model, it may need more time to finish training but can not improve the F1-score.\n",
    "\n",
    "The effect of regularisation strength and the relationship between learning rate and the number of epochs is same as task1.\n",
    "\n",
    "| Example | Learing Rate  | Alpha | Epochs | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  | 0.06  | 1e-5  | >30  | 0.781|\n",
    "| Example1  | 5  | 1e-5  | 2  | 0.203|\n",
    "| Example2  | 1e-5  |  1e-5 | >30   | 0.339|\n",
    "| Example3  | 0.06  |  1e-9 | 2   | 0.137|\n",
    "| Example4  | 0.06  |  1 | >30   | 0.181|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now repeat the training and evaluation process for BOW-tfidf, BOCN-count, BOCN-tfidf, BOW+BOCN including hyperparameter tuning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### BOW-tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_idf = []\n",
    "number_documents = len(topic_train_np)\n",
    "for word in topic_train_vocab:\n",
    "    df = topic_train_df.get(word)\n",
    "    topic_idf.append(np.log(number_documents/df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_matrix = topic_train_tfmatrix * np.array(topic_idf)\n",
    "tfidf_dev_matrix = topic_dev_tfmatrix * np.array(topic_idf)\n",
    "tfidf_test_matrix = topic_test_tfmatrix * np.array(topic_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▌                                                                             | 2/30 [00:00<00:07,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w_count, training_loss_history, validation_loss_history = SGD(tfidf_train_matrix,topic_train_np[:,0], tfidf_dev_matrix, \n",
    "                                                               topic_dev_np[:,0],num_classes=3,lr=0.06,alpha=1e-6,\n",
    "                                                              epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7766666666666666\n",
      "Precision: 0.7758086699405858\n",
      "Recall: 0.7766666666666667\n",
      "F1-Score: 0.7758064994920028\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(topic_test_tfmatrix)\n",
    "label = np.array(topic_test_np[:,0].tolist())\n",
    "predicted = predict_class(X_te_count, w_count)\n",
    "preds_te = list()\n",
    "Y_te = list()\n",
    "for i in range(len(label)):\n",
    "    Y_te.append(int(label[i]))\n",
    "    preds_te.append(int(predicted[i]))\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### BOCN-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2400/2400 [00:00<00:00, 3305.52it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_trainbocn_vocab, topic_trainbocn_df, topic_trainbocn_counts = get_vocab(topic_train_np[:,1], \n",
    "                                                                              token_pattern = \"[^0-9A-Za-z\\u4e00-\\u9fa5]\", \n",
    "                                                                            stop_words=stop_words, min_df=5,char_ngrams=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2400/2400 [00:00<00:00, 5715.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 900/900 [00:00<00:00, 5502.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 2785.16it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_trainbocn_extract_ngrams = whole_extracy_ngrams(topic_train_np[:,1], char_ngrams=True, \n",
    "                                                      token_pattern = \"[^0-9A-Za-z\\u4e00-\\u9fa5]\")\n",
    "topic_testbocn_extrace_ngrams = whole_extracy_ngrams(topic_test_np[:,1], char_ngrams=True, \n",
    "                                                     token_pattern = \"[^0-9A-Za-z\\u4e00-\\u9fa5]\")\n",
    "topic_devbocn_extrace_ngrams = whole_extracy_ngrams(topic_dev_np[:,1], char_ngrams=True, \n",
    "                                                    token_pattern = \"[^0-9A-Za-z\\u4e00-\\u9fa5]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2400/2400 [00:05<00:00, 443.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 900/900 [00:02<00:00, 416.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 462.77it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_trainbocn_tfmatrix = vectorise(topic_trainbocn_extract_ngrams, topic_trainbocn_vocab)\n",
    "topic_testbocn_tfmatrix = vectorise(topic_testbocn_extrace_ngrams, topic_trainbocn_vocab)\n",
    "topic_devbocn_tfmatrix = vectorise(topic_devbocn_extrace_ngrams, topic_trainbocn_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|███████████████████▎                                                               | 7/30 [00:03<00:10,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Bocn_count, Bocn_training_loss_history, Bocn_validation_loss_history = SGD(topic_trainbocn_tfmatrix, topic_train_np[:,0], \n",
    "                                                                        topic_devbocn_tfmatrix, topic_dev_np[:,0],\n",
    "                                                                        lr=0.04,epochs=30,tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8133333333333334\n",
      "Precision: 0.8127753616039274\n",
      "Recall: 0.8133333333333334\n",
      "F1-Score: 0.8128597159952116\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(topic_testbocn_tfmatrix)\n",
    "label = np.array(topic_test_np[:,0].tolist())\n",
    "predicted = predict_class(X_te_count, Bocn_count)\n",
    "preds_te = list()\n",
    "Y_te = list()\n",
    "for i in range(len(label)):\n",
    "    Y_te.append(int(label[i]))\n",
    "    preds_te.append(int(predicted[i]))\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOCN-tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_bocnidf = []\n",
    "number_documents = len(topic_train_np)\n",
    "for word in topic_trainbocn_vocab:\n",
    "    df = topic_trainbocn_df.get(word)\n",
    "    topic_bocnidf.append(np.log(number_documents/df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bocntfidf_train_matrix = topic_trainbocn_tfmatrix * topic_bocnidf\n",
    "bocntfidf_dev_matrix = topic_devbocn_tfmatrix * topic_bocnidf\n",
    "bocntfidf_test_matrix = topic_testbocn_tfmatrix * topic_bocnidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████▏                                                            | 8/30 [00:02<00:07,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Bocntfidf_count, Bocntfidf_training_loss_history, Bocntfidf_validation_loss_history = SGD(bocntfidf_train_matrix, topic_train_np[:,0], \n",
    "                                                                        bocntfidf_dev_matrix, topic_dev_np[:,0],\n",
    "                                                                        num_classes=3, lr=0.04,epochs=30,\n",
    "                                                                        tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7788888888888889\n",
      "Precision: 0.783246601494355\n",
      "Recall: 0.7788888888888889\n",
      "F1-Score: 0.7777364866071256\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(bocntfidf_test_matrix)\n",
    "label = np.array(topic_test_np[:,0].tolist())\n",
    "predicted = predict_class(X_te_count, Bocntfidf_count)\n",
    "preds_te = list()\n",
    "Y_te = list()\n",
    "for i in range(len(label)):\n",
    "    Y_te.append(int(label[i]))\n",
    "    preds_te.append(int(predicted[i]))\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW(tfidf)+BOCN(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_bocn_train = np.hstack((tfidf_train_matrix,bocntfidf_train_matrix))\n",
    "bow_bocn_dev = np.hstack((tfidf_dev_matrix, bocntfidf_dev_matrix))\n",
    "bow_bocn_test = np.hstack((tfidf_test_matrix, bocntfidf_test_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 6/30 [00:02<00:10,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bowbocn_count, bowbocn_training_loss_history, bowbocn_validation_loss_history = SGD(bow_bocn_train, topic_train_np[:,0], \n",
    "                                                                        bow_bocn_dev, topic_dev_np[:,0],\n",
    "                                                                        num_classes=3,lr=0.04,epochs=30,\n",
    "                                                                        tolerance=0.001, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8144444444444444\n",
      "Precision: 0.8150107367156267\n",
      "Recall: 0.8144444444444444\n",
      "F1-Score: 0.8138749837252869\n"
     ]
    }
   ],
   "source": [
    "X_te_count = np.array(bow_bocn_test)\n",
    "label = np.array(topic_test_np[:,0].tolist())\n",
    "predicted = predict_class(X_te_count, bowbocn_count)\n",
    "preds_te = list()\n",
    "Y_te = list()\n",
    "for i in range(len(label)):\n",
    "    Y_te.append(int(label[i]))\n",
    "    preds_te.append(int(predicted[i]))\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:16:42.567569Z",
     "start_time": "2020-02-15T14:16:42.562560Z"
    }
   },
   "source": [
    "## Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  | 0.783  | 0.781  | 0.781  |\n",
    "| BOW-tfidf  | 0.776  | 0.777  | 0.776  |\n",
    "| BOCN-count  | 0.813  | 0.813  | 0.813  |\n",
    "| BOCN-tfidf  | 0.783  | 0.779  | 0.778  |\n",
    "| BOW+BOCN  | 0.815  | 0.815  | 0.814  |\n",
    "\n",
    "Please discuss why your best performing model is better than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model is BOW+BOCN. It maight have sereval reasons:\n",
    "1) BOW(tfidf)+BOCN(tfidf) has more features than other 4 models which can representive the sentence better.\n",
    "\n",
    "2) Both of BOW and BOCN have use tfidf to give weights to each features which can achieve better performance than only use count. TFIDF = TF*IDF and IDF can reduce the importance of words that appear more frequency in the document in the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
